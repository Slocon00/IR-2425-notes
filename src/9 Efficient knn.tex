\chapter{Efficient k-Approsimated Nearest Neighbors Retrieval}

K-Nearest Neighbors (k-NN) is an algorithm used to solve classification and regression tasks. Given a set of examples, the algorithm calculates the output of a new instance by finding the $k$ nearest examples and aggregating their target values: the output is the majority class in the case of classification, or the average value in the case of regression. In information retrieval, k-NN can be used on embedded data to find the most similar documents to a query. Formally, the task is defined as follows: given a set of $n$ document embeddings $D = \{d_1, d_2, \dots,d_n\}$, and given a query $q$, find
\begin{equation*}
    \arg\min_{d_i \in D}^{(k)} s(q, d_i) 
\end{equation*}
where $s(q,v)$ is a distance measure (e.g., euclidean distance, inverse of the dot product). Those $k$ documents can be then returned as a result to the user as the most relevant.

\sloppy
In practice, real applications use a variant called \textbf{k-Approximated Nearest Neighbors} (\textbf{k-ANN}) for two reasons: exact k-NN is computationally expensive, since the number of documents in collections can be very large, and it suffers from the \textbf{``curse of dimensionality''}, a phenomenon where as the number of dimensions in the data increases, distances become less and less meaningful. To illustrate the problem, consider the following examples. First, consider a 1-dimensional space of embeddings with $n$ points. Given a query, the 1-nearest neighbor can be calculated very easily: if euclidean distance is used as a measure, then the document in question is always either the predecessor or the successor of the query, which can be found with a binary search in $\Theta(\log n)$ time. If dot product is used, then the document is either the smallest (if the document has a negative value), or the maximum (if the query is positive).
\clearpage
\begin{figure}[t]
    \centering
    \includesvg[width=0.8\textwidth]{img/knn_1d.svg}
    \caption{Example of k-NN in a 1-dimensional space.}
\end{figure}
\fussy
In a 2-dimensional space, the problem becomes more complex. Given the document embeddings, the solution is to partition the space using a \textbf{Voronoi diagram}: the space is divided into a number of regions, each centered around a document, so that all points falling in the same region are closer to the center than to ani other document. If there are $n$ points, the total number of vertices and edges in the diagram is $\Theta(n)$. Then, to find the nearest neighbor, the algorithm must find the region in which the embedding of the query falls into: this can be done in $\Theta(\log n)$ time, and linear space.

Now, let's generalize to a $d$-dimensional space. The Voronoi diagram has size $O(n^{d/2})$. For the search, there are two alternatives: either the algorithm performs a point query, in $\Theta((d + \log n)^c)$ time and $\Theta(n^d)$ space, or a linear scan, in $\Theta(nd)$ time and linear space. Either way, the complexity grows with the number of dimensions, and the algorithm becomes impractical for high-dimensional data.

Possible strategies for k-ANN retrieval are tree-based solutions, clustering-based solutions, hashing-based solutions, and proximity graphs. The next section will focus on the latter, as they are the state-of-the-art solutions for high-dimensional data.

\section{Proximity Graphs and Hierarchical Navigable Small World}

A proximity graph is a graph where each node is a document and each edge between two nodes represents some relationship of proximity between the documents they represent. \textbf{Navigable Small World} (\textbf{NSW}) graphs are a type of proximity graph characterized by a high clustering coefficient and low distances between nodes. Given a set of points, the NSW graph is constructed by consecutively adding elements extracted at random, connecting them to the $m$ closest neighbors chosen from the points already present in the graph. Once the graph is constructed, the algorithm can find the nearest neigbor(s) of a query by navigating the graph with a greedy search algorithm: the search starts from a random entry node, and, at each step, moves to the neighbor that is closest to the query. The search stops when the nearest neighbor is found (meaning that the distance between the current node and the query is smaller than the distance with its neighbors), or when none of the current node's neighbors are closer to the query than the current node itself.

An improvement over NSW is \textbf{Hierarchical Navigable Small World} (\textbf{HNSW}) graphs, which introduce a hierarchy of nodes to increase the chances of success and potentially speed up the search. As the graph is being constructed, each new node is also assigned a layer, randomly chosen from a scale with an exponentially deacaying probability distribution (the higher the layer, the lower the chances of being chosen). A proximity graph is constructed at each layer, and any node in a layer $l$ is also an entry point for the graph in the layer $l-1$, allowing vertical movement across the hierarchy. The greedy search algorithm is modified to start from an element in the top layer, and gradually move closer and closer to the query by navigating the graph either in the current layer, or by moving to a lower layer when appropriate.
\begin{figure}[h]
    \centering
    \includesvg[width=0.8\textwidth]{img/hnsw.svg}
    \caption{Example of a greedy search in 1-dimensional data to find the predecessor of the query using a HNSW graph. The blue arrows and nodes represent the direction of the search, the red nodes are the neighbors that are larger than the query, and the green node is the predecessor.}
\end{figure}

\section{Product Quantization}

Product quantization is a lossy compression technique often used together with HNSW graphs to reduce the space usage of the data and make distance computations faster. This technique breaks up a $d$-dimensional vector into $m$ chunks of size $d/m$ each, and replaces each chunk with a code of 1 byte. 
First, all documents are split into $m$ chunks. Then, $k$-means is used to find 256 centroids among the chunks; the $k$-means algorithm is a clustering algorithm that partitions the data into $k$ clusters, producing a Voronoi tessellation. Finally, each chunk is associated to the centroid closest to it, and the code of the chunk is the index of the centroid.
\begin{figure}[h]
    \centering
    \includesvg[width=0.70\textwidth]{img/prod_quantization_1.svg}
    \caption{Example of product quantization of a 12-dimensional vector, split into 4 chunks.}
\end{figure} \\
To perform a $k$-NN search, the query is also split into $m$ chunks; the distance between the query and a document can then be approximated as the sum of the distances between the chunks of the query and the centroids to which the chunks of the documents have been associated with.
\begin{figure}[h]
    \centering
    \includesvg[width=0.75\textwidth]{img/prod_quantization_2.svg}
    \caption{Example of search. The query is compared to the centroids only, and the documents are not decompressed.}
\end{figure}

\section{Neural Information Retrieval and k-ANN}

Large language models encode documents and queries into embeddings, which can be of three types: dense single-vector, dense multi-vector, and sparse single-vector embeddings. The last type is the most efficient in terms of time and space, and is interpretable by design (since each dimension of the vector corresponds to some feature of the document, its presence or absence is immediarely evidend and comparable with others). This section will illustrate how $k$-NN searches can be done on sparse vectors, and ways to make them more efficient.

Assume we have $n$ documents and $m$ different features in the dataset. Realistically, $n$ can be in the order of millions, and $m$ in the order of tens/hundreds of thousands. Since the data is sparse, each document has very little non-zero components (around few hundreds). If we built a forward index, pairing each document ID to the list of non-zero components it has, we'd have some million rows, and some hundreds of entries per row. Now, we assume to have a query with $m'$ non-zero components ($m'$ will also be pretty small), and we want to find the document vectors that are most similar to it (e.g., maximize the dot product). The brute force approach consists in scanning the forward index to retrieve the document vectors one at a time, calculating the dot product with the query vector, and selecting the $k$ documents that maximize the product. This approach is  very slow, as the number of dot products to calculate is exactly $n$.

We've previously seen how inverted indexes can be used to efficiently retrieve documents in a collection. We can build an inverted index where each row is a feature associated to the list of documents where that feature is non-zero. The total number of rows is $m$, and the number of entries per row is a fraction of $n$. To perform the search, the index is scanned in a TAAT fashion, going through the lists associated with the non-zero entries of the query, and calculating the dot product only with the documents in those lists. This approach is faster than the previous one, but is still inefficient, as the number of dot products to calculate is $O(m'n)$.

\textbf{Seismic} is an algorithm that allows effective and efficient approximate retrieval. The key idea behind this algorithm is the ``concentration of importance'' property of the dot product: by taking the top 10\% of the largest components in queries and documents, the 85\% (on average) of the dot product is preserved. This property is used to limit the size of the inverted index. When component $i$ is inserted in the index, all the IDs of the documents that have a non-zero value in $i$ are added to its list. Then, the IDs are sorted in descending order, by the value of component $i$ in the respective documents. That way, the documents at the start of the list are those with the highest component value, and those at the end are those with the lowest value. The lists of the index are then truncated, keeping only the first $\lambda$ entries for a fixed $\lambda$. When a search is made, only the top $\sigma$ components of the query are considered for the scan of the inverted index ($\sigma$ is also a fixed hyperparameter).

This pruning already greatly improves efficiency, but the algorithm is still forced to evaluate all the documents in each list of the top $\sigma$ components. Seismic uses a form of dynamic pruning to significantly reduce the number of documents to evaluate through blocking and summaries. \textbf{Blocking} consists in partitioning each inverted list into $\beta$ groups, so that a group contains documents that are similar to each other (this is done via a clustering algorithm). For each group, a \textbf{summary} is built to ``represent'' it, so it can be used to quickly skip the group if the dot product between the query and the summary is too low.

The summary can be built by simply taking the maximum value of each component across all the documents in that block. This summary is always conservative, meaning that its dot product with the query is always greater or equal than the dot product with any document in the group. However, the number of non-zero entries in the summary grows quickly with the block size. To address this problem, the summary itself is also pruned by only keeping the largest non-zero components: because of the ``concentration of importance'' property, the dot product will still be precise enough even if no longer conservative, while the number of components to consider is greatly reduced.
\begin{figure}[h]
    \vspace{10pt}
    \centering
    \includesvg[width=0.9\textwidth]{img/seismic_summaries.svg}
    \caption{Example of how summaries are built in Seismic. The left side shows the original summary, the right side shows the pruned summary.}
\end{figure}