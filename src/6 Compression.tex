\chapter{Data Compression}

Modern database systems are distributed across multiple physical nodes in a network. This distribution is necessary to handle the massive document collections held by search engines and to improve query processing performances. \textbf{Sharding} is a technique used to split data into smaller chunks, to be then assigned to the nodes of a network. Sharding ensures not only scalability and performance, but also fault tolerance, since shards are duplicated across different nodes to prevent data loss or slow-downs in case of node failures. 

Indexes are also sharded. The assignment of shards to nodes can be represented as a matrix, where each column is a shard, and each row is a node. When a query must be processed, a component called \textbf{broker} submits the query to different rows (machines) trying to balance the load, considering which nodes have which shards. The bigger the index is, the more shards are needed; the more users are querying the system, the more replicas are needed. In both cases, it is useful to increase the number of machines.

A way to reduce the number of shards needed to partition the index is to use \textbf{compression}. Compression algorithms are either lossless (i.e., the compressed data can be decompressed to the original form with no loss of information) and lossy (i.e., the compression process loses some information, which becomes unrecoverable). Either way, the process must be reversible, providing a function that can fully or almost fully restore the original data.

The quality of a compression algorithm can be measured in terms on \textbf{compression ratio}, which is the ratio between the size of the original data and the size of the compressed data:
\begin{equation*}
    \textit{CR} = \frac{|B|}{|C(B)|}
\end{equation*}
If $\textit{CR} = r$, then the size of the compressed output is $r$ times smaller than the input size. The compression ratio can be chosen to find the best tradeoff between compression speed, amount of energy spent, loss of precision, etc. Ultimately, an important goal is to find a compression algorithm that allows direct computation over compressed data without the need for decompression.

\section{Lossless Compression}

\subsection{Integer Encoders}

We are given an integer $x > 0$, and want an algorithm, called \textbf{code}, that represents $x$ in as few as possible bits. The output of the code is a bit-string called \textbf{codeword}, indicated as $C(x)$. A message $L = [x_1, \dots , x_n]$ consisting of $n$ integers can be coded as the concatenation of the codewords of each integer: $C(L) = [C(x_1), \dots, C(x_n)]$. The codes we will see next are \textbf{static}, meaning they always assign the same codeword to the same integer regardless of the message to be coded.

Before applying any actual compression algorithm, the posting list can be preprocessed to reduce the value of the document IDs within them. Starting from the second element in the list, each docID is replaced by the difference between it and the previous one. This technique is called \textbf{d-gaps}.
\begin{figure}[!ht]
    \centering 
    \includesvg[width=0.70\textwidth]{img/dgaps.svg} 
    \caption{Example of d-gaps.}
    \label{fig:dgaps}
\end{figure}

\paragraph{Binary Encoding}
Binary encoding converts each number to a binary string of fixed length. Given $\textit{bin}(x)$ the function that returns the binary representation of $x$, the binary codewords are calculated as:
\begin{equation*}
    B(x) = \textit{bin}(x-1)
\end{equation*}
This way, each number can be represented using $|B(x)| = \ceil{log_2(x)}$ bits (the trailing zeros are removed). This major problem of this technique is that when the codewords are concatenated together to form the message, there is no way of telling where a codeword starts and where it ends: this leads to an ambiguous representation that does not have a unique decoding. The source of ambiguity is caused by the fact that the code $B()$ is not \textbf{prefix-free}: a code $C$ is said to be prefix-free if there are no $x, y$ s.t. $C(x) \leq C(y)$ for which $C(x) = C(y)[1 : |C(x)|]$. In other words, this compression can make it so that some codewords are the prefix of other, longer codewords.

\paragraph{Unary Encoding}
Unary encoding produces sequences of bits set to 1, and uses 0 as the delimeter between each sequence. Each number is coded as:
\begin{equation*}
    U(x) = 1^{x-1}0
\end{equation*}
i.e., a sequence of $x-1$ 1s and a 0. Therefore, $|U(x)| = x$. This method is good only for small integers, since the size of the compressed output grows linearly with that of the uncompressed input.

\paragraph{Elias Gamma and Delta Encoding}
The key idea on which the Elias Gamma encoding is based is to use the binary representation of a number, and prepend the unary encoding of its length. The encoding function is as follows:
\begin{equation*}
    \gamma(x) = U(|\textit{bin}(x)|) \textit{bin}(x)[2:]
\end{equation*}
The first bit is removed from the binary representation, since it is always equal to 1. For example, $\gamma(4) = 11 \ 0 \ 00$. The first two bits mean that the following word is 3 bits long; the next 0 bit is a delimiter; the last two bits are the binary encoding of number 4 (note that the first 1 bit was removed from 100).

Elias Delta encoding is built on top of Gamma. It still prepends the length of the binary representation, but it is further compressed using Elias Gamma. The code function is:
\begin{equation*}
    \delta(x) = \gamma(|\textit{bin}(x)|) \textit{bin}(x)[2:]
\end{equation*}
Using the same example as before, $\delta(4) = \gamma(3) 00 = 10 \ 1 \ 00$. The first two bits mean that the binary representation of 3 is long 2 bits, the next bit is the binary representation of 3 (without the leading 1), and finally, the last two bits are the binary representation of 4 (again without the leading 1).

These methods can be extended one upon the other indefinitely; each further extension produces codewords that are shorter for bigger numbers, and longer for shorter numbers. Additionally, if a method chains $m$ compressions, we also need to do $m$ decompressions, thus increasing the time needed.

\paragraph{Variable Byte Encoding}

Variable Byte encoding produces codewords that are byte-aligned instead of bit-aligned; byte-alignment favors implementation simplicity and encoding/decoding speed at the cost of lower compression ratio. 

This method uses the binary encoding of a number, and splits it into bytes; for each byte, the 7 least significant ones are used for the representation itself (\textbf{data bits}), while the most significant bit (\textbf{control bit}) is used to signal continuation (1) or end (0) of that representation. For example, $VB(4) = 0 \ 0000011$. It only takes one byte since it's small. On the other hand, a large number can require multiple bytes: $\textit{VB}(267) = 1 \ 0000010 \ 0 \ 0001010$.

\subsection{List Encoders}

List encoders are used to compress an entire integer list instead of individual integers. The goal is to compress a list $L = [x_1, \dots, x_n]$ of strictly increasing integers, where $x_1 > 0$ and $x_n \leq U$. $U$ is called universe size. Some compressors operate directly on the list, while others first transform the list into a sequence of d-gaps.

As a way to analyze the space effectiveness of the compressors is to compare them to the combinatorial lower bound, meaning the minimum number of bits needed to represent a list of $n$ strictly increasing integers drawn out of a universe of size $U \geq n$. There are $\binom{U}{n}$ ways of randomly choosing $n$ distinct numbers out of $U$ possibilities; therefore, we need at least $\ceil{\log_2 \binom{U}{n}}$ bits. By Stirling's approximation, we can estimate the lower bound as:
\begin{equation*}
    \ceil{\log_2 \binom{U}{n}} \approx n (\log_2 \left(\frac{U}{n}\right) + \log_2 e) \approx n \log_2 \left(\frac{U}{n}\right) + 1.44 n \,.
\end{equation*}
In practice, however, many compressors actually take less space than this lower bound. This is because the lower bound assumes that the integers are truly randomly distributed, but in real applications, lists of document IDs tend to contain many clusters of close integers, and this closeness can be exploited. For example, in an inverted index built for a search engine, the word ``Pisa'' may contain a lot of IDs of documents relating to the University of Pisa, which are likely to be all close to each other.

\paragraph{Simple9 and Simple16}

These two methods are based on the same principle: pack as many integers as possible in a memory word (32 or 64 bits long). Both operate on d-gaps. Simple9 adopts 32-bit words and has 9 ``configurations'', while Simple16 has 16 of them.

Simple 9 dedicates 4 bits to the \textbf{selector code} and the remaining 28 for the data. The selector indicates how many elements are packed in the segment using equally sized codewords. The possible configurations are described in the table below:

\begin{table}[!ht]
    \centering
    \begin{tabular}{c|c|c}
        \textbf{Selector} & \textbf{Integers} & \textbf{Bits per Integer} \\
        \hline
        \hline
        0000 & 28 & 1 \\
        0001 & 14 & 2 \\
        0010 & 9 & 3 \\
        0011 & 7 & 4 \\
        0100 & 5 & 5 \\
        0101 & 4 & 7 \\
        0110 & 3 & 9 \\
        0111 & 2 & 14 \\
        1000 & 1 & 28 \\    
    \end{tabular}
    \caption{Simple9 selector codes.}
    \label{tab:simple9}
\end{table}
For example, the list $L = 1^{33} \ 3^5 \ 2^5 \ 1^2 \ 7^6$ is encoded with three 32-bit words as $1^{28} \ - \ 1^5 \ 3^5 \ 2^4 \ - \ 2^1 \ 1^2 \ 7^6$. The very first block has selector $0000$, since the only integer present is 1; the second block has selector $0001$, since we need two bits to represent 3 and 2; the last block has selector $0010$, since we need three bits to represent 7.

\paragraph{Patched Frame of Reference}

A big limitation of block-based strategy is their inefficiency when a block contains only a single large value, because it forces a large amount of bits to be used for the codeword: in the example above, had the last block contained the number 512 instead of 7, it would have forced the use of 9 bits for any other number in the same block (which, for small numbers such as 1 or 2, would have big a huge waste of space).

Patched Frame of Reference (PFor) tries to solve this issue by choosing a base $b$ and a value $k > 0$ for the universe representation of the block, such that a large percentage (e.g., 90\%) of the block's integers fall in the range $[b, b+ 2^k - 1]$, i.e., they can be represented using $k$ bits. Any integer that does not fit into $k$ bits is treated as an \textbf{exception} and encoded in a separate array using some other compression algorithm.

\paragraph{Binary Interpolative Coding}

Suppose we have a list of integers $L$ such that $l \leq L[1]$ and $L[n] \leq h$. Then, we can encode the element in the middle of the sequence, $L[m]$ (where $m = \ceil{n/2}$), knowing it is in the range $[l,h]$. Instead of encoding this number as it is, we encode $L[m] - l$ using $\ceil{\log_2(h-l)}$ bits. For example, the list is $L = [2, 3, 5, 7, 9, 10, 13]$, then $l = 2$ and $h = 13$. The middle element is 7, and we can encode it as $7 - 2 = 5$. If we apply the same encoding to the two halves $L[1:m-1]$ and $L[m+1:n]$, we now have updated knowledge of who the upper and lower bounds are for each half. In the example, the two sublists are $L_1 = [2, 3, 5]$ ($l_1 = 2, h_1 = 6$) and $L_2 = [9, 10, 13]$ ($l_2 = 8, h_2 = 13$).

The crucial property of this method is that whenever the length $r$ of the interval is equal to $h - l$, then a run of $r$ consecutive integers can be detected and no bits are necessary: for this reason, binary interpolative coding is particularly effective on highly clustered sequences.

\paragraph{Elias-Fano Encoding}

Let $S(n,U)$ be a sorted sequence of $n$ integers drawn from a universe of size $U$. Each integer $S[i]$ is converted in its binary representation $\textit{bin}(S[i])$, which takes $\ceil{\log_2 U}$ bits; this representation is split in two parts:
\begin{itemize}
    \item The \textbf{low part} containing the low bits, which are the rightmost $l = \ceil{\log_2(U/n)}$ bits;
    \item The \textbf{high part} containing the high bits, which are the remaining $h = \ceil{\log_2 U} - l$ bits.
\end{itemize}
\begin{figure}[!ht]
    \centering
    \includesvg[width=0.70\textwidth]{img/elias_fano_split.svg}
    \caption{Example array split into high and low parts.}
    \label{fig:elias-fano-split}
\end{figure} 
The encoding of $S(n,U)$ is given by the encoding of the high and low parts of all the integers. The low parts of the integers, $[l_1, \dots, l_n]$, are written as they are in a bitvector $L$ of $n \ceil{\log_2 (U/n)}$ bits. The high parts $[h_1, \dots, h_n]$ are also represented with a bitvector, of $n + 2^{\floor{\log_2 n}} \leq 2n$ bits. We start from a 0-valued bitvector $H$, and set the bit in position $h_i + i, \ \forall i = 1, \dots, n$. The end result is a vector that contains the cardinalities of the ``buckets'' in which the bits of the high part can be grouped into. This vector is composed of $\leq 2n$ bits because it contains a 1 for each number in the sequence, and no more than a 0 for each possible bucket (at most $n$).
\begin{figure}[H]
    \centering
    \includesvg[width=0.70\textwidth]{img/elias_fano_H.svg}
    \caption{Vector H obtained from the example above. Note how the first bucket has two elements, the second also has two elements, the third has one element, and so on: these cardinalities are stored in sequence, separated by one or more 0s.}
    \label{fig:elias-fano-H}
\end{figure}
\noindent Finally, the Elias-Fano representation of $S$ is the concatenation of $H$ and $L$, and overall takes
\begin{equation*}
    EF(S(n,U)) \leq n \ceil{\log_2 (U/n)} + 2n
\end{equation*}
bits.

Elias-Fano supports random access without decompression. To access an element, we want to implement the operation \texttt{Access}$(i)$ which returns $S[i]$. We assume we can efficiently use the \texttt{Select$_0$}$(i)$ and  \texttt{Select$_1$}$(i)$ primitives, which return, respectively, the position of the $i^{th}$ 0 and 1 in a bitvector. Using \texttt{Select$_1$}$(i)$, it is possible to implement \texttt{Access}$(i)$ in $O(1)$ time. To reconstruct a number, we need to re-link the two halves of its binary representation: the low part is easily retrieved, as it is the range of bits $L[(i-1) * l + 1, i * l]$. The retrieval of the high bits requires a few steps; the bitvector $H$ tells us how many integers share the same high part and fall in the same bucket, expressed in unary. To find exactly who are these bits for a given position in the original sequence, we simply need to know how many 0s are encountered before the $i^{th}$ 1 in $H$; the position of this bit is retrieved using \texttt{Select$_1$}$(i)$. If we remove $i$ from that position (essentially doing the inverse operation used to build the bitvector), we find the original value of the high bits. By concatenating them to the low bits found before, the whole number is reconstructed.

As an example, consider Figures \ref{fig:elias-fano-split} and \ref{fig:elias-fano-H}. If we wanted to find the $4^{th}$ element of the sequence, we would first look at the low bits, stored as they are, reading from position $7$ to $8$ (finding bits $11$). Then, we look at $H$, and find the position of the $4^{th}$ 1, which is 5. We subtract 4 from this position, obtaining 1, i.e., $001$ in bit representation. Finally, we concatenate the two halves and find that the number in the $4^{th}$ position is $00111 = 7$.

\paragraph{Partitioned Elias-Fano Encoding}

As mentioned above in PFor, it's common for posting lists to have clusters of consecutive document IDs. Elias-Fano does not exploit the presence of these clusters, requiring the same amount of bits to encode a list regardless of its structure. Partitioned Elias-Fano encoding is a variant that first partitions each posting list into chunks of variable length, and then separately encodes each chunk using Elias-Fano. The optimal partitioning that minimizes the size of compressed data can be found in linear time uning a dynamic programming algorithm.
