\chapter{Natural Language}

Natural language is the language used by humans to communicate with each other. Languages are defined on the basis of thousands of words, a complex syntax, and a mostly compositional semantic. Language is also often ambiguous; the same word may have different meanings depending on the context of the sentence, or the same sentence may be interpreted in different ways.

\textbf{Natural Language Understanding} has the aim of building machines capable of receiving and giving information using natural language, like a human would. Natural language processing is said to be an ``AI-complete'' problem, meaning that it is a problem only solvable using AI, and that if we were to find a solution, then we'd have a solution for any other problem. In practice, natural language processing is used in many applications, such as chatbots, search engines, machine translation, and personal assistants (such as Alexa by Amazon, or Google Home by Google). It can be used with different data types: tabular data (to construct and interpret search queries), graphs (to represent the content of each node and link, for example in a social media graph), and images/video data (to describe the content of the image/video and to retrieve or generate similar images/videos).

Information Retrieval is strictly connected and overlapping with the fields of Natural Language Processing and Machine Learning:
\begin{itemize}
    \item NLP methods are often built on top of IR or ML methods;
    \item ML uses IR measures to define the goals of the learned models, and assumes language can be manipulated via NLP.
\end{itemize}

\section{NLP Pipelining}

A \textbf{processing pipeline} is a sequence of preprocessing steps aimed at transforming the raw text input into a format that can be effectively used as input to the chosen machine learning model(s). Some of the key steps in the pipeline are:
\begin{itemize}
    \item \textbf{Tokenization}: it identifies the words (tokens) in the text. Popular libraries provide ``language aware'' tokenization, i.e., it isolates tokens differently depending on the language or context of the text. After this step, a \textbf{vocabulary} of the terms used can be constructed.
    \item \textbf{Sentence splitting}: it isolates whole sentences from the text, so that they can be analyzed individually. This is not always an easy task, as punctuation marks can often be used for uses other than sentence separation (e.g., in acronyms, numbers, initials, etc.).
    \item \textbf{Stemming and lemmatization}: both aim at reducing words to their roots; stemming does it by applying a set of language-dependent transformation rules to find the stem of a word, while lemmatization actually tries to find the root of the word in the vocabulary. The difference between the two can be shown with an example: the word ``cars'' is both stemmed and lemmatized to ``car''; the word ``was'' may be stemmed to ``wa'', but lemmatized to ``be''.
\end{itemize}
Raw text can be transformed in different formats depending on the task to be solved. Common models are bag-of-words and n-grams.

\textbf{Bag-of-words} represents a text as the list of unique words appearing in it. It loses any information about word frequency and order, requiring external data structures to store this information; on the other hand, it is easy to implement and calculate. The set of all distinct extracted words can be called \textit{dictionary}, \textit{vocabulary}, or \textbf{feature set/space}, since each word becomes a feature of the new representation.

\textbf{N-grams} features are sequences of $n$ words which capture word order. They do not correspond to a specific token in the text, and are instead obtained as a combination of them. For example, in the text:
\begin{center}
    \textit{``the quick brown fox jumps over the lazy dog''}
\end{center}
we can find 2-grams such as:
\begin{center}
    \textit{'the quick'', 'quick brown', 'brown fox', ... , 'the lazy', 'lazy dog'}
\end{center}
Different libraries have specific formats to identify n-grams.

\section{Zipf's Law}

Zipf's Law is an empirical law used to model the frequency of words in a text. It is based in the observation that the most common word in a text is usually twice as frequent as the second most common one, three times as frequent as the third most common, and so on. Specifically, the law states that the frequency of a word is proportional to the inverse of the rank:
\begin{equation*}
    \textit{frequency} = \frac{1}{(\textit{rank} + b)^a}
\end{equation*}
where $a \approx 1$, $b \approx 2.7$.

\begin{figure}[ht]
    \centering
    \includesvg[width=0.75\textwidth]{img/zipf.svg}
    \caption{Plots showing the frequency of words (in the English language) according to Zipf's Law.}
    \label{fig:zipf}
\end{figure}

The \textbf{principle of least effort} is a theory that states that animals, people, and even well-designed machines naturally choose the path of least effort, meaning the one that requires the least amount of work to reach a goal. In the case of human communication, both speaker and listener in a conversation will abide by this principle. The speaker tends to use a small vocabulary of common words, while the listener tends to prefer longer, rarer words. Zipf's Law can be seen as the result to the compromise between the two.

\textbf{Stopwords} are the most common words in a language. In the English lanaguage, stopwords include ``the'', ``a'', ``to''. They can be removed from the text without losing too much information. Different libraries and tools provide their own list of stopwords depending on the application; for example, MySQL specifies words such as ``appreciate'' or ``unfortunately'', since it's used for sentiment analysis.

When it comes to rare words, it is not necessarily true that uncommon words are the most informative or useful. If a word is so rare that it appears very few times in very few documents, it may actually be of little help for future retrieval: it could be a typo, or a sort of artificial identifier associated to the document (e.g., a slug in an url). Removing these rare words can help make it faster to process indexed data, and requires less space.

\section{Vector Space Model}

Words can be represented as $|F|$-dimensional vectors obtained through \textbf{one-hot encoding}, where $F$ is the set of distinct features (words, n-grams, etc.) in the collection. The relevance of a feature $f$ in the document $d$ is indicated as $w_{f,d}$.

A document can then be represented as the weighted sum of all the vectors of its features:
\begin{equation*}
    v(d) = \sum_{f \in d} w_{f,d} v(f)
\end{equation*} 
These vector representations are usually sparse (most of their terms are equal to 0). Weights can be assigned in different ways. Common methods are binary weights (bag-of-words), term frequency (tf), and tf-idf.

To find matches between a query and a document, we can compare the terms appearing in both: using a simple boolean operator (AND/OR), we can check wheter a certain document contains only/all the terms in the query and return them to the user. This is a very simple methodology, but it does not produce a ranking and does not take into account the frequency of words.

In \textbf{ranked retrieval}, the system reorders the (top $k$) documents in the collection for a given query. Instead of defining binary queries with a specific language, natural language is used instead. To rank documents, we need some way to assign a \textbf{score} to each document given a query that measures how well they match.

A possible scoring is given by \textbf{Jaccard's coefficient}:
\begin{equation*}
    \textit{jaccard}(A,B) = \frac{|A \cap B|}{|A \cup B|}
\end{equation*}
where $A$ and $B$ are sets (in our case, the query and the document). This coefficient is always a value between 0 and 1, where 0 means no overlap and 1 means perfect overlap. While a better alternative than basic binary scoring, it is based on the assumpion that its operators are sets, so it does not consider term frequency to calculate the score. The more frequent is the term in the document, the higher the score; the rarer the term in a collection, the more informative it is (with the exceptions mentioned above).

\subsection{TF-IDF Scoring}

We introduce two data structures to then define more sophisticated scoring measures: the incidence matrix and the count matrix. \\
The \textbf{incidence matrix} is used to represent the presence of absence of terms in documents. Each row corresponds to a word, and each column corresponds to a document.
\begin{table}[!ht]
    \centering
    \begin{tabular}{c|cccc}
        & $d_1$ & $d_2$ & $d_3$ & $d_4$ \\
        \hline
        $w_1$ & 1 & 0 & 1 & 0 \\
        $w_2$ & 0 & 1 & 0 & 1 \\
        $w_3$ & 1 & 1 & 0 & 0 \\
        $w_4$ & 0 & 0 & 1 & 1 \\
    \end{tabular}
    \caption{Example of an incidence matrix.}
    \label{tab:incidence-matrix}
\end{table} \\
The \textbf{count matrix} keeps track of the number of occurrences of a term in a document, so each column of the matrix is a count vector, and each count vector is a multiset.
\begin{table}[!ht]
    \centering
    \begin{tabular}{c|cccc}
        & $d_1$ & $d_2$ & $d_3$ & $d_4$ \\
        \hline
        $w_1$ & 32 & 0 & 5 & 0 \\
        $w_2$ & 0 & 15 & 0 & 13 \\
        $w_3$ & 2 & 1 & 0 & 0 \\
        $w_4$ & 0 & 0 & 10 & 2 \\
    \end{tabular}
    \caption{Example of a count matrix.}
    \label{tab:count-matrix}
\end{table} \\
Note that this representation still does not consider the order in which words appear in the documents.

The \textbf{term frequency} $\textit{tf}_{t,d}$ of a term $t$ in a document $d$ is simply the number of times $t$ appears in $d$. Raw term frequency cannot be used as is, however, since relevancy is not linearly proportional to it. A document with 10 occurrences of a term is more relevant than a document with only 1 occurrence, but it is not 10 times more relevant. \textbf{Log-frequency weight} is a common way to normalize term frequency:
\begin{equation*}
    w_{t,d} = \begin{cases}
        1 + \log_{10}(\textit{tf}_{t,d}) & \textit{tf}_{t,d} > 0 \\
        0 & \text{otherwise}
    \end{cases}
\end{equation*} 
The score of a document-query pair is then calculated by summing the weights of all the terms appearing in both:
\begin{equation*}
    \textit{score}(q,d) = \sum_{t \in q \cap d} w_{t,d}
\end{equation*}
The score is always 0 if none of the terms of the query appear in the document.

However, we previously mentioned that informativeness also depends on how frequent the word is in the entire collection. If a term is globally rare, documents containing that word will be more relevant for queries asking for it. To capture this aspect, \textbf{document frequency} $\textit{df}_t$ is used: it is defined as the number of documents containing term $t$. Since $\textit{df}_t$ measures the inverse of the informativeness of a word, \textbf{inverse document frequency} ($\textit{idf}_t$) is used instead:
\begin{equation*}
    \textit{idf}_t = \log_{10} \left(\frac{N}{\textit{df}_t}\right)
\end{equation*}
where $N$ is the number of documents in the collection. The logaritm is used for the same reason as for term frequency. Inverse document frequency by itself cannot rank documents in one-term queries, since $\textit{idf}$ is always either 0 if the document does not contain the term, or a positive value if it does (obtained by the formula above). For multi-term queries, scoring is given by:
\begin{equation*}
    \textit{score}(q,d) = \sum_{t \in q \cap d} \textit{idf}_t
\end{equation*}
In this formula, the role of the document is to filter out the terms which contribute to its own ranking.

These two measures are combined into what's known as \textbf{tf-idf weighting}, which is the product between them:
\begin{equation*}
    \textit{tf-idf}_{t,d} = w_{t,d} \cdot \textit{idf}_t
\end{equation*}
This is the most common weighting scheme in Information Retrieval. Scoring of a document-query pair is calculated as:
\begin{equation*}
    \textit{score}(q,d) = \sum_{t \in q \cap d} \textit{tf-idf}_{t,d}
\end{equation*}
There are many variants which differ in the way the components are weighted, or the way single terms in the query are weighted as well. \\
A frequently used variant is \textbf{Best Match 25} (\textbf{BM25}), which calculates the score as:
\begin{gather*}
    \textit{score}(q,d) = \sum_{t \in q \cap d} \textit{idf}_t \cdot \frac{\textit{tf}_{t,d} \cdot (k_1 + 1)}{\textit{tf}_{t,d} + k_1 \cdot (1 - b + b \cdot \frac{|D|}{\textit{avgdl}})} \\
    \textit{idf}_t = \ln \left( \frac{N - \textit{df}_t + 0.5}{\textit{df}_t + 0.5} + 1 \right)
\end{gather*}
where $|D|$ is the length of the document (how many words it contains), \textit{avgdl} is the average length of the documents in the collection, and $b, k_1$ are hyperparameters; $b \in [0,1]$ and $k_1 \in [1.2,2.0]$.

\subsection{Vector Space Similarity Scoring}

Since \textit{tf-idf} can be calculated for each term-document pair, we can construct an additional data structure, called \textbf{weight matrix}, which stores all these values. Each document (which is a column in the matrix) can be then thought of as a vector representing a point in $\mathbb{R}^{|F|}$.

The scoring introduced above, however, still presents a problem. Ideally, we want to consider not only the relevancy of single words, but also how relevant a document is in the entire collection. If a document has many rare terms, it should rank higher than another document which has less rare terms. To solve this issue, we can rank  documents according to how similar/distant they are from the query (which can be represented as a $|F|$-dimensional vector as well).

A first similarity measure is \textbf{Euclidean distance}. It is simple to calculate, and does return a value that is greater the more distant the two vectors are in the space considered. However, Euclidean distance can end up being way too high for vectors of different lenghts (i.e., unnormalized), even if they have the same orientation: for example, consider a document $d$ and append it to itself, obtaining $d'$. Even though the two documents have the same content (and will have the same direction in the vector space), their magnitude is different, with $d'$'s being exacly double that of $d$. The key idea is then to rank documents depending on the angle forming between them.

Ranking documents in decreasing order of the angle between query and document is equivalent to ranking them in increasing order of the cosine of the angle. This measure is called \textbf{cosine similarity}:
\begin{gather*}
    \cos(\theta) = \frac{q \cdot d}{\|q\| \cdot \|d\|} = \frac{\sum_{i=1}^{|F|} q_i d_i}{\sqrt{\sum_{i=1}^{|F|} q_i^2} \cdot \sqrt{\sum_{i=1}^{|F|} d_i^2}}
\end{gather*}
Note that here vectors are length-normalized by dividing each for its L2 norm. In the previous example, $d$ and $d'$ would end up having the same lenght after being normalized, so their cosine similarity would be 1 (the maximum possible value).

Different search engines can use different weighting and different normalizations to calculate \textit{tf-idf} and vector similarity. In the \textbf{SMART} notation, each method is associated to a different letter (e.g., n for natural/no weighting, l for logarithm, b for boolean, etc.), and the combination used by the engine is specified by a string of six letters: \textit{ddd.qqq}. Different weightings can be used for queries and documents. A standard scheme is \textbf{\textit{lnc.ltc}}: logarithmic \textit{tf}, no \textit{idf}, cosine similarity normalization for documents, and logarithmic \textit{tf} \textit{idf}, and cosine similarity normalization for queries.