\chapter{Natural Language}

Natural language is the language used by humans to communicate with each other. Languages are defined on the basis of thousands of words, a complex syntax, and a mostly compositional semantic. Language is also often ambiguous; the same word may have different meanings depending on the context of the sentence, or the same sentence may be interpreted in different ways.

\textbf{Natural Language Understanding} has the aim of building machines capable of receiving and giving information using natural language, like a human would. Natural language processing is said to be an ``AI-complete'' problem, meaning that it is a problem only solvable using AI, and that if we were to find a solution, then we'd have a solution for any other problem. In practice, natural language processing is used in many applications, such as chatbots, search engines, machine translation, and personal assistants (such as Alexa by Amazon, or Google Home by Google). It can be used with different data types: tabular data (to construct and interpret search queries), graphs (to represent the content of each node and link, for example ina social media graph), and images/video data (to describe the content of the image/video and to retrieve or generate similar images/videos).

Information Retrieval is strictly connected and overlapping with the fields of Natural Language Processing and Machine Learning:
\begin{itemize}
    \item NLP methods are often built on top of IR or ML methods;
    \item ML uses IR measures to define the goals of the learned models, and assumes language can be manipulated via NLP.
\end{itemize}

\section{NLP Pipelining}

A \textbf{processing pipeline} is a sequence of preprocessing steps aimed at transforming the raw text input into a format that can be effectively used as input to the chosen machine learning model(s). Some of the key steps in the pipeline are:
\begin{itemize}
    \item \textbf{Tokenization}: it identifies the words (tokens) in the text. Popular libraries provide ``language aware'' tokenization, i.e., it isolates tokens differently depending on the language or context of the text. After this step, a \textbf{vocabulary} of the terms used can be constructed.
    \item \textbf{Sentence splitting}: it isolates whole sentences from the text, so that they can be analyzed individually. This is not always an easy task, as punctuation marks can often be used for uses other than sentence separation (e.g., in acronyms, numbers, initials, etc.).
    \item \textbf{Stemming and lemmatization}: both aim at reducing words to their roots; stemming does it by applying a set of language-dependent transformation rules to find the stem of a word, while lemmatization actually tries to find the root of the word in the vocabulary. The difference between the two can be shown with an example: the word ``cars'' is both stemmed and lemmatized to ``car''; the word ``was'' may be stemmed to ``wa'', but lemmatized to ``be''.
\end{itemize}
Raw text can be transformed in different formats depending on the task to be solved. Common models are bag-of-words and n-grams.

\textbf{Bag-of-words} represents a text as the list of unique words appearing in it. It loses any information about word frequency and order, requiring external data structures to store this information; on the other hand, it is easy to implement and calculate. The set of all distinct extracted words can be called \textit{dictionary}, \textit{vocabulary}, or \textbf{feature set/space}, since each word becomes a feature of the new representation.

\textbf{N-grams} features are sequences of $n$ words which capture word order. They do not correspond to a specific token in the text, and are instead obtained as a combination of them. For example, in the text:
\begin{center}
    \textit{``the quick brown fox jumps over the lazy dog''}
\end{center}
we can find 2-grams such as:
\begin{center}
    \textit{'the quick'', 'quick brown', 'brown fox', ... , 'the lazy', 'lazy dog'}
\end{center}
Different libraries have specific formats to identify n-grams.

\section{Zipf's Law}

Zipf's Law is an empirical law used to model the frequency of words in a text. It is based in the observation that the most common word in a text is usually twice as frequent as the second most common one, three times as frequent as the third most common, and so on. Specifically, the law states that the frequency of a word is proportional to the inverse of the rank:
\begin{equation*}
    \textit{frequency} = \frac{1}{(\textit{rank} + b)^a}
\end{equation*}
where $a \approx 1$, $b \approx 2.7$.

The \textbf{principle of least effort} is a theory that states that animals, people, and even well-designed machines naturally choose the path of least effort, meaning the one that requires the least amount of work to reach a goal. In the case of human communication, both speaker and listener in a conversation will abide by this principle. The speaker tends to use a small vocabulary of common words, while the listener tends to prefer longer, rarer words. Zipf's Law can be seen as the result to the compromise between the two.

\textbf{Stopwords} are the most common words in a language. In the English lanaguage, stopwords include ``the'', ``a'', ``to''. They can be removed from the text without losing too much information. Different libraries and tools provide their own list of stopwords depending on the application; for example, MySQL specifies words such as ``appreciate'' or ``unfortunately'', since it's used for sentiment analysis.

When it comes to rare words, it is not necessarily true that uncommon words are the most informative or useful. If a word is so rare that it appears very few times in very few documents, it may actually be of little help for future retrieval: it could be a typo, or a sort of artificial identifier associated to the document (e.g., a slug in a url). Removing these rare words can help make it faster to process indexed data, and requires less space.

\section{Vector Space Model}

Words can be represented as $|F|$-dimensional vectors obtained through \textbf{one-hot encoding}, where $F$ is the set of distinct features (words, n-grams, etc.) in the collection. The relevance of a feature $f$ in the document $d$ is indicated as $w_{f,d}$.

A document can then be represented as the weighted sum of all the vectors of its features:
\begin{equation*}
    v(d) = \sum_{f \in d} w_{f,d} v(f)
\end{equation*} 
These vector representations are usually sparse (most of their terms are equal to 0). Weights can be assigned in different ways. Common methods are binary weights (bag-of-words), term frequency (tf), and tf-idf.

To find matches between a query and a document, we can compare the terms appearing in both: using a simple boolean operator (AND/OR), we can check wheter a certain document contains only/all the terms in the query and return them to the user. This is a very simple methodology, but it does not produce a ranking and does not take into account the frequency of words.

\subsection{Vector Space Ranking}

In \textbf{ranked retrieval}, the system reorders the (top $k$) documents in the collection for a given query. Instead of defining binary queries with a specific language, natural language is used instead. To rank documents, we need some way to assign a \textbf{score} to each document given a query that measures how well they match.

A possible scoring is given by \textbf{Jaccard's coefficient}:
\begin{equation*}
    \textit{jaccard}(A,B) = \frac{|A \cap B|}{|A \cup B|}
\end{equation*}
where $A$ and $B$ are sets (in our case, the query and the document). This coefficient is always a value between 0 and 1, where 0 means no overlap and 1 means perfect overlap. While a better alternative than basic binary scoring, it is based on the assumpion that its operators are sets, so it does not consider term frequency to calculate the score. The more frequent is the term in the document, the higher the score; the rarer the term in a collection, the more informative it is (with the exceptions mentioned above).

We introduce two data structures to then define more sophisticated scoring measures: the incidence matrix and the count matrix. \\
The \textbf{incidence matrix} is used to represent the presence of absence of terms in documents. Each row corresponds to a word, and each column corresponds to a document.
\begin{table}[!ht]
    \centering
    \begin{tabular}{c|cccc}
        & $d_1$ & $d_2$ & $d_3$ & $d_4$ \\
        \hline
        $w_1$ & 1 & 0 & 1 & 0 \\
        $w_2$ & 0 & 1 & 0 & 1 \\
        $w_3$ & 1 & 1 & 0 & 0 \\
        $w_4$ & 0 & 0 & 1 & 1 \\
    \end{tabular}
    \caption{Example of an incidence matrix.}
    \label{tab:incidence-matrix}
\end{table} \\
The \textbf{count matrix} keeps track of the number of occurrences of a term in a document, so each column of the matrix is a count vector, and each count vector is a multiset.
\begin{table}[!ht]
    \centering
    \begin{tabular}{c|cccc}
        & $d_1$ & $d_2$ & $d_3$ & $d_4$ \\
        \hline
        $w_1$ & 32 & 0 & 5 & 0 \\
        $w_2$ & 0 & 15 & 0 & 13 \\
        $w_3$ & 2 & 1 & 0 & 0 \\
        $w_4$ & 0 & 0 & 10 & 2 \\
    \end{tabular}
    \caption{Example of a count matrix.}
    \label{tab:count-matrix}
\end{table} \\
Note that this representation still does not consider the order in which words appear in the documents.

The \textbf{term frequency} $\textit{tf}_{t,d}$ of a term $t$ in a document $d$ is simply the number of times $t$ appears in $d$. Raw term frequency cannot be used as is, however, since relevancy is not linearly proportional to it. A document with 10 occurrences of a term is more relevant than a document with only 1 occurrence, but it is not 10 times more relevant. \textbf{Log-frequency weight} is a common way to normalize term frequency:
\begin{equation*}
    w_{t,d} = \begin{cases}
        1 + \log_{10}(\textit{tf}_{t,d}) & \textit{tf}_{t,d} > 0 \\
        0 & \text{otherwise}
    \end{cases}
\end{equation*} 
The score of a document-query pair is then calculated by summing the weights of all the terms appearing in both:
\begin{equation*}
    \textit{score}(q,d) = \sum_{t \in q \cap d} w_{t,d}
\end{equation*}
The score is always 0 if none of the terms of the query appear in the document.

However, we previously mentioned that informativeness also depends on how frequent the word is in the entire collection. If a term is globally rare, documents containing that word will be more relevant for queries asking for it. To capture this aspect, \textbf{document frequency} $\textit{df}_t$ is used: it is defined as the number of documents containing term $t$. Since $\textit{df}_t$ measures the inverse of the informativeness of a word, \textbf{inverse document frequency} ($\textit{idf}_t$) is used instead.
